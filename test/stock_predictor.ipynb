{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH # if /usr/local/cuda/bin is missing, re-run VScode form terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ptxas --version # expecting 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as r\n",
    "\n",
    "data = pd.read_csv('./stock_bars.csv')\n",
    "data = data.loc[data.symbol == r.choice(data.symbol.unique())] # pick a single stock from the test data each time\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data:pd.DataFrame, skip_cols:list[str]=[]) -> pd.DataFrame:\n",
    "    skip_cols = [col for col in skip_cols if col in data.columns]\n",
    "    skip = data[skip_cols]\n",
    "    temp = data.drop(skip_cols, axis=1)\n",
    "    temp = (temp - temp.mean()) / temp.std(ddof=0) # standardize\n",
    "    temp = temp.ffill().fillna(0) # impute\n",
    "    return pd.concat([skip, temp], axis=1, join='inner')\n",
    "\n",
    "norm = normalize(data, ['symbol', 'timestamp'])\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = norm['low'].shift(-1) - norm['low'] # since data is normalized, we attempt to learn/predict the difference between t-1 and t\n",
    "X = norm.drop(['symbol', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = int(len(X)*0.8) # 80:20 train/test split\n",
    "# X_train, X_test = X[:cutoff], X[cutoff:-1]\n",
    "# y_train, y_test = y[:cutoff], y[cutoff:-1]\n",
    "\n",
    "# print(len(X), len(X_train), len(X_test))\n",
    "# print(len(y), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clique\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = len(X.columns)\n",
    "ACTIVATION_1 = 'tanh' # inputs are standardized so keep negative range\n",
    "ACTIVATION_2 = 'relu' # performed better than tanh, sigmoid\n",
    "DROPOUT = 0.5         # performed better than 0.3, 0.4\n",
    "RANDOM_STATE = 25     # funnier than 24\n",
    "\n",
    "layers = tf.keras.layers\n",
    "Sequential = tf.keras.Sequential\n",
    "regularizer = tf.keras.regularizers.l1(0.001)\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "shared_kw = dict(random_state=RANDOM_STATE, learning_rate=0.2, max_depth=3, subsample=0.8)\n",
    "xgb_lgb_kw = dict(n_jobs=16, colsample_bytree=0.85, reg_alpha=500)                         \n",
    "xgb_cat_kw = dict(early_stopping_rounds=5)\n",
    "lgb_cat_kw = dict(num_leaves=8, min_child_samples=2000)\n",
    "\n",
    "models = [ # order matters if limit is set; frontloading stronger models will cause more rejections; the reverse will oversaturate\n",
    "    xgb.XGBRegressor(**shared_kw, **xgb_lgb_kw, **xgb_cat_kw, eval_metric='mae', tree_method='hist', gamma=0.2), #, nthread=1),\n",
    "    lgb.LGBMRegressor(**shared_kw, **xgb_lgb_kw, **lgb_cat_kw, early_stopping_round=5, metric='l1', min_split_gain=0.001, verbosity=-1),\n",
    "    cat.CatBoostRegressor(**shared_kw, **xgb_cat_kw, **lgb_cat_kw, eval_metric='MAE'),\n",
    "    Sequential([layers.Dense(1, activation=ACTIVATION_1, input_shape=[N_FEATURES])], name='linear'), # N -> 1\n",
    "    # Sequential([ # N -> N/2 -> 1\n",
    "    #     layers.Dense(N_FEATURES, kernel_regularizer=regularizer, activation=ACTIVATION_1, input_shape=[N_FEATURES]),\n",
    "    #     layers.Dropout(DROPOUT),\n",
    "    #     layers.BatchNormalization(),\n",
    "    #     layers.Dense(N_FEATURES//2, kernel_regularizer=regularizer, activation=ACTIVATION_2),\n",
    "    #     layers.Dropout(DROPOUT),\n",
    "    #     layers.BatchNormalization(),\n",
    "    #     layers.Dense(1)\n",
    "    # ], name='net'),\n",
    "]\n",
    "\n",
    "# try: ensemble = e.load_ensemble(X_test=X, y_test=y)\n",
    "# except FileNotFoundError: \n",
    "ensemble = clique.train_ensemble(models, X, y, folds=5, limit=3)\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
